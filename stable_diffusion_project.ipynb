{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4Wt5nGAa7K",
        "outputId": "7cc49c98-fa69-43f5-e155-ed05c404c44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohU83whD_jXa"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'cifar_data/'"
      ],
      "metadata": {
        "id": "ZUb0aMchAhYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading cifar10 dataset and applying custom transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),     #resizing images to 64x64 pixels\n",
        "    transforms.ToTensor(),           #converting images to tensors for efficient tensor computations using pytorch/tensorflow\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  #normalizing to [-1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "tljfcQdiAr8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We convert the images into tensors so they can be an input into a neural network for training or other image processing purposes. Each image is composed of pixels which contains information regarding color or intensity, 2 values for grayscale images (single scalar) and 3 for color images (red, green, blue) representing the intensity of of each color channel. The image is treated like a grid which is reshaped using 'rows' and 'columns' which correspond to the height and width of the image. Once processed the resulting pixel values are respresented as multi-dimensional arrays (tensors). For grayscale images - 2D tensors (height, width) and for color images - 3D tensor (height, width, color channels). Here we further process the pixels by normalizing them into values between 0 and 1 for improved precision during training, to reduce potential data skew which can affect training outcomes and to make the inputs consistent which keeps interpretation consistent as well, duing training of the model**"
      ],
      "metadata": {
        "id": "pm9AACfvEQSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True) #cifar10 dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6KBr8VLA70W",
        "outputId": "bb079926-f8b5-439f-fc29-01cf06108640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 68009451.36it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting cifar_data/cifar-10-python.tar.gz to cifar_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "UKSRx8nJBNLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 #batch size for dataloader\n",
        "dataloader = torch.utils.data.DataLoader(cifar_dataset, batch_size=batch_size, shuffle=True) #dataloader for batch processing"
      ],
      "metadata": {
        "id": "Mq-KLqdMA_Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator module with fully connected layers\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "52wSeMQxBEZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**noise_dim = dimensionality of the input noise vector - in context of image generation the random noise vector is used as the initial seed for the generator to produce an image\n",
        "image_channels - specifies the number of channels in the images that are generated, grayscale images have this value set to 1 and color images have this value set to 3. It determines how many color or intensity channels each pixel in the generated image will have.\n",
        "hidden_dim - represents the number of hidden units or neurons in the intermediate layers of the generator network. the hidden layers transform the input noise vector into a complex representation that can ultimately be decoded into an image**"
      ],
      "metadata": {
        "id": "aIwRkAvUNkiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, noise_dim, images_channels, hidden_dim=64):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    #fully connected layers transform noise into images\n",
        "    self.fc=nn.Sequential(\n",
        "        nn.Linear(noise_dim, hidden_dim * 8 * 4 * 4), #performs linear transformation (maps the input noise into a higher dimension space)\n",
        "        nn.ReLU(inplace=True), #rectified linear unit activation function introduces non-linearity into the network\n",
        "        nn.BatchNorm1d(hidden_dim * 8 * 4 * 4), #batch normalization layer which normalizes the activations of the previous layer ensuring mean activation is close to 0 and sd is close to one\n",
        "        nn.Unflatten(1, (hidden_dim * 8, 4, 4)), #reshapes the tensor by taking the flattened tensor from the previous layer and reshaping it into a 4x4 feature map w/ hidden_dim*8 channels\n",
        "        nn.ConvTranspose2d(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding=1), #transposed convolutional layer\n",
        "        nn.ReLU(inplace=True), #relu activation applied after the transposed layer\n",
        "        nn.BatchNorm2d(hidden_dim * 4), #bath normalization applied after the second convolutional layer\n",
        "        nn.ConvTranspose2d(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=2, padding=1), #transposed convolutional layer reducing no. channels from hidden_dim*4 to hidden_dim*2\n",
        "        nn.ReLU(inplace=True), # relu activation applied after third convolutional layer\n",
        "        nn.BatchNorm2d(hidden_dim * 2), #batch normalization after third convolutional layer\n",
        "        nn.ConvTranspose2d(hidden_dim * 2, images_channels, kernel_size=4, stride=2, padding=1), #final transposed convolutional layer\n",
        "        nn.Tanh()  #output in the range [-1, 1] for images\n",
        "    )\n",
        "  def forward(self, noise):\n",
        "    return self.fc(noise)\n",
        "\n"
      ],
      "metadata": {
        "id": "DnCWq4sFNCsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "E4lU-mCPYZRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel(nn.Module):\n",
        "  def __init__(self, generator, num_steps):\n",
        "    super(DiffusionModel, self).__init__()\n",
        "\n",
        "    self.generator = generator #generator network\n",
        "    self.num_steps = num_steps #number of diffusion steps\n",
        "\n",
        "  def forward(self, noise):\n",
        "    intermediates = [] #initialize a list to store intermediate images\n",
        "\n",
        "    for step in range(self.num_steps):\n",
        "      #generating image at current step\n",
        "      image = self.generator(noise)\n",
        "      intermediates.append(image) #append intermediate image\n",
        "\n",
        "      #adding diffusion noise to the noise vector\n",
        "      noise = self.add_diffusion_noise(noise, step, self.num_steps)\n",
        "\n",
        "    return intermediates #returning a list of generated images at each diffusion step\n",
        "\n",
        "  def add_diffusion_noise(self, noise, steps, num_steps):\n",
        "    noise_magnitude = steps/num_steps #linearly increase noise magnitude from 0 to 1 over the diffusion steps\n",
        "\n",
        "    # generate spatial noise pattern using gaussian blur\n",
        "    spatial_noise = self.generate_gaussian_blur(noise.shape[-2:])\n",
        "\n",
        "    #combine spatial and temporal noise\n",
        "    noisy_noise = noise * noise_magnitude + spatial_noise\n",
        "\n",
        "    #noisy input added to input noise\n",
        "    noisy_input = noise + noisy_noise\n",
        "\n",
        "    return noisy_input\n",
        "\n",
        "  def generate_gaussian_blur(self, image_size, max_blur=3):\n",
        "    #random gaussian blur pattern\n",
        "    blur_rad = np.random.uniform(0, max_blur)\n",
        "\n",
        "    blurred_img = cv2.GaussianBlur(np.random.rand(*image_size), (0,0), (blur_rad))\n",
        "    return blurred_img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sS7ff6PoLW-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian Blur is a widely used image and processing technique that is applied to an image to reduce image noise and detail while preserving the overall structure of the image**"
      ],
      "metadata": {
        "id": "0aEjaDOGfAHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**G(x, y) = (1 / (2πσ^2)) * exp(-(x^2 + y^2) / (2σ^2))**"
      ],
      "metadata": {
        "id": "pvOQwrTrfP0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fields for text and summary\n",
        "SRC = Field(tokenize=\"spacy\", tokenizer_language=\"en\", init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "TGT = Field(tokenize=\"spacy\", tokenizer_language=\"en\", init_token=\"<sos>\", eos_token=\"<eos>\")\n"
      ],
      "metadata": {
        "id": "TgoXKXp5Y7oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVFLeqTQjQbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}